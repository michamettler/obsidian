#machinelearning #sem5 

https://phonosync.github.io/mldm-notes/02_linear_regression.html
https://github.com/michamettler/zhaw-mldm/blob/main/L02/L02_Linear_Regression_ASSIGNMENT.ipynb
## Univariate Linear Regression
One of the simplest cases of model-based learning isÂ **Univariate Linear Regression**Â (also calledÂ **Simple Linear Regression**). Here, the output depends only on one input variableÂ x, and the hypothesis is a linear combination of two parametersÂ $\theta_0$Â andÂ $\theta_1$Â and the input valueÂ $x$:
$$h_{\theta_0,\theta_1}(x)=\theta_0+\theta_1 x$$
Note that in the literature,Â $\hat{y}$Â is usually used for the predicted value (as computed by the model), whereasÂ $y$Â would be used for the true value.
### Loss Function
For linear regression, theÂ **Residual Sum of SquaresÂ RSS**Â (also calledÂ **Sum of Residuals**Â orÂ **Sum of Squared Errors**) is typically used
$$â„’_{RSS}(ğœƒ_0,ğœƒ_1;\{x^{(m)},y^{(m)}\}) = \sum_{ğ‘š=1}^ğ‘€(ğ‘¦^{(m)}âˆ’\hat{ğ‘¦}^{(m)})^2 = \sum_{ğ‘š=1}^M \varepsilon_ğ‘š^2$$
with
$$\varepsilon^{(m)} = y^{(m)} - \hat{y}^{(m)}$$
### Cost Function
The learning (training) consists of minimizing aÂ **Cost FunctionÂ $J$**, which is a function of the model parameters and depends implicitly on the training data. In the simplest case, the cost function is equal to the loss function and a normalising factor for mathematical convenience:
$$J(ğœƒ_0,ğœƒ_1)=\frac{1}{2 M} \sum_{ğ‘š=1}^ğ‘€(ğ‘¦^{(m)}âˆ’\hat{ğ‘¦}^{(m)})^2 \tag{2.1}$$
Minimizing the cost function results in specific values for the model parametersÂ $\theta_0$Â andÂ $\theta_1$, which can then be used for predicting the output for new data samples (also calledÂ **Inference**):
$$\hat{y}=h_{\hat{\theta}_0,\hat{\theta}_1}(x)=\hat{\theta}_0+\hat{\theta}_1 x$$
### Closed Form Solution for Univariate Linear Regression
Using these equations we can simple compute the optimal values for parametersÂ $\theta_0$Â andÂ $\theta_1$ Â for theÂ $M$Â given training samples ${(ğ‘¥^{(m)},ğ‘¦^{(m)})}$.
$$\hat{\theta}_0=\mu_y-\theta_1\mu_x \ $$
and
$$\hat{\theta}_1=\frac{\sum_{m=1}^M(x^{(m)}-\mu_x)(y^{(m)}-\mu_y)}{\sum_{m=1}^M(x^{(m)}-\mu_x)^2}= \frac{\tilde{s}_{xy}}{\tilde{s}_x^2} \tag{2.2}$$
#### Example Calculation $\theta_1$
Data Set:
![[Pasted image 20241006145655.png#invert]]
Calculation (vgl. $2.2$):
![[Pasted image 20241006145723.png#invert]]
#### Example Calculation Python

```python
class UnivariateLinearRegression:

  def __init__(self):
    self.theta_0: float = 0.
    self.theta_1: float = 0.

  def predict(self, x):
    y_pred = self.theta_0 + self.theta_1 * x
    return y_pred

  def fit(self, x, y):
    x_mean = np.mean(x)
    y_mean = np.mean(y)

    num = np.sum((x - x_mean) * (y - y_mean))
    denom = np.sum((x - x_mean) ** 2)

    self.theta_1 = num / denom
    self.theta_0 = y_mean - self.theta_1 * x_mean

    return self

lin_reg_uni = UnivariateLinearRegression()
lin_reg_uni.fit(x, y1)
y_pred = lin_reg_uni.predict(x)

def plot_model(x, y_pred, y_true):
  # scatter plot of the true data points
  plt.scatter(x, y_true)
  # plot the fitted line
  plt.plot(x, y_pred, c="r")
  # label axes
  plt.xlabel("x")
  plt.ylabel("y")
  plt.show()

plot_model(x=x, y_pred=y_pred, y_true=y1)
```
### Normal Equation
Alternatively to the approach resulting inÂ $2.2$ we can use the computation of the residuals to derive a closed form for computing the optimal parameters.
$$\begin{pmatrix}
\varepsilon^{(1)} \\
\varepsilon^{(2)} \\
\vdots \\
\varepsilon^{(M)}
\end{pmatrix}=
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(M)}
\end{pmatrix}
-
\begin{pmatrix}
1 & x^{(1)} \\
1 & x^{(2)} \\
1 & \vdots \\
1 & x^{(M)}
\end{pmatrix}
\begin{pmatrix}
\theta_0 \\
\theta_1
\end{pmatrix}$$
or shorter as
$$\boldsymbol{\varepsilon} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\theta}$$
with a corresponding matrix $X$, where the first column contains valuesÂ $1$Â and the second column contains the input values from the training set, andÂ yÂ contains the corresponding expected output values.

This leads to a closed form-expression for the optimal parameters:
$$\boldsymbol{\theta} = (\boldsymbol{X}^T \boldsymbol{X})^{âˆ’1} \boldsymbol{X}^T \boldsymbol{y} \tag{2.3}$$
We can now use thisÂ **Normal Equation**Â to directly compute the solution for a linear regression problem.

## Multivariate Linear Regression
**Multivariate Linear Regression**, also calledÂ **Multiple Linear Regression**, is the natural extension of the simple linear regression framework to more than one predictor variables (features).

Thus, we now extend our simple approach for one input variable to the general case withÂ $N$Â input variables (features). TheÂ $N>1$Â features can be combined into a single linear combination. The model then takes the following form for theÂ $m$â€™th sample (we denote theÂ $i$â€™th feature of theÂ $m$â€™th sample asÂ xi(m)):
$$\hat{y}^{(m)} = h_{\theta}(x^{(m)}) = \theta_0 x_{0}^{(m)} + \theta_1 x_{1}^{(m)} + \theta_2 x_{2}^{(m)} + ... + \theta_N x_{N}^{(m)}  = \boldsymbol{\theta}^T\boldsymbol{X}_{m,:}  \tag{2.4}$$
ere, we introduce a new artificial feature variableÂ $x_{0}^{(m)} := 1$Â for all $m=1,...,M$ to simplify the notation (i.e., writing the sum as a vector product).
$$\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\theta} + \boldsymbol{\varepsilon}$$
#### Example
![[Pasted image 20241006151311.png#invert]]
This can be expressed using the corresponding matrices:
$$\boldsymbol{X}=\begin{pmatrix}
1 & 29932.5 & 4.8 & 70160 & 18 \\
1 & 31007.8 & 1 & 104458 & 16.4 \\
1 & 32181.2 & 1 & 232666 & 16.9
\end{pmatrix}
\ \ \
\boldsymbol{\theta} = \begin{pmatrix}
\theta_0 \\
\theta_1 \\
\theta_2 \\
\theta_3 \\
\theta_4
\end{pmatrix}
\ \ \
\boldsymbol{y} = \begin{pmatrix}
5.9 \\
5.6 \\
5.4
\end{pmatrix}$$
#### Example Calculation Python
```python
import time
sizes = []
times = []
n = 1000
for m in [10, 50, 100, 200, 250, 500, 1000, 10000]:
  X, y = create_random_data(m, n)

  start_time = time.monotonic()

  theta = (np.linalg.inv(X.T @ X) @ X.T) @ y
  # theta = np.linalg.pinv(X) @ y
  # theta = np.linalg.solve(X.T @ X, X.T @ y)

  elapsed = time.monotonic() - start_time

  problem_size = m

  sizes.append(problem_size)
  times.append(elapsed)

plt.scatter(sizes, times)
plt.xlabel('Problem-Size')
plt.ylabel('Runtime (s)')
plt.show()
```
## Residual Plots
![[Pasted image 20241006151454.png#invert]]
## Basic Assumptions
### Linearity:
The relationship betweenÂ $X$Â andÂ $y$Â is linear.
![[Pasted image 20241006151545.png#invert]]

### Independence:
The residuals are independent of each other.
![[Pasted image 20241006151604.png#invert]]
### Normality:
The expected output values are normally distributed.
![[Pasted image 20241006151625.png#invert]]
### Homoscedasticity (equality of variance):
The variance of the residual is the same for any value ofÂ $X$
![[Pasted image 20241006151652.png#invert]]
## Evaluating Regression Models
There are several metrics that can be used to quantify how well the expected and predicted output values correspond:
### Mean Absolute Error (MAE)
$$MAE=\frac{\sum_{ğ‘–=1}^ğ¼|ğ‘¦^{(i)}âˆ’\hat{ğ‘¦}^{(i)}|}{ğ¼}$$
### Mean Squared Error (MSE)
$$MSE=\frac{\sum_{ğ‘–=1}^ğ¼(ğ‘¦^{(i)}âˆ’\hat{ğ‘¦}^{(i)})^2}{ğ¼}$$
#### Example Python
```python
def mse(y_pred, y_true):
    return np.mean((y_pred - y_true)**2)
```
### Root Mean Squared Deviation (RMSD)
$$ğ‘…ğ‘€ğ‘†ğ·=\sqrt{MSE}=\sqrt{\frac{\sum_{ğ‘–=1}^ğ¼(ğ‘¦^{(i)}âˆ’\hat{ğ‘¦}^{(i)})^2}{ğ¼}}$$
withÂ $I$ number of samples in the independent test set.