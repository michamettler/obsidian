#machinelearning #sem5

https://phonosync.github.io/mldm-notes/04_polynomial_regression.html
## Polynomial Models
How can we deal with data where the relation is not linear, for instance as shown in the figure below? In this case, we can try to fit aÂ **Polynomial Model**Â to the data, i.e.Â instead of a linear hypothesis, we use a polynomial hypothesis. For instance, for the data in the figure we might use a polynomial of degree 3, i.e.
![[Pasted image 20241006153427.png#invert]]
$$h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$$
To formalize this, we first defineÂ **Artificial Variables**
$$z_1 = x, z_2 = x^2, z_3 = x^3$$
Using these, we can reformulate our hypothesis as
$$h_{\theta}(z) = \theta_0 z_0 + \theta_1 z_1 + \theta_2 z_2 + \theta_3 z_3$$
Hence, we can apply our methods from before - e.g.Â normal equation ([[2 Linear Regression]]), [[3 Gradient Descent]] etc. - to solve these problems.
For **Multivariate Linear Regression Model** (multiple input variables $x_1, .. x_n$) ([[2 Linear Regression]]):
$$h(x_1, .. x_n) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2x _3 + \theta_3 x_2 x_3^3 + \theta_4 \sqrt{x_2 x_3} +...$$
## Over- and Underfitting
If we take very large degrees, our [[Learning Curve]] will â€œjump aroundâ€ a lot. While this model would have very good costs (close to zero), it is not really useful, since it â€œdoes not generalizeâ€ well (for new values) => **Overfitting**.
![[Pasted image 20241006153732.png#invert]]
### Regularization
In order to avoid overfitting, we can either decrease the degree of the polynomials - or we can, for a given degree, prevent the fitted [[Learning Curve]] from â€œjumping aroundâ€ too much.
$$J(\theta) = \frac{1}{2 M} \left[ \sum_{ğ‘š=1}^ğ‘€(ğ‘¦^{(m)}âˆ’h_{\theta}(x^{(m)}))^2 + \lambda \sum_{j = 1}^{n} \theta_j^2 \right] \tag{4.1}$$
![[Pasted image 20241006153910.png#invert]]
Note that the regularization term dependsÂ _only_Â on the parametersÂ $\theta_0$, not on the training samples. TheÂ **Hyperparameter**Â $\alpha$Â determines â€œhow muchâ€ regularization counts: The largerÂ $\alpha$Â , the more the values ofÂ $\theta_0$Â contribute to the cost.
#### Example Python different degrees
```python
def calcWithPol(pol):
    poly = PolynomialFeatures(degree=pol)
    X_poly_train = poly.fit_transform(X_train)
    X_poly_test = poly.fit_transform(X_test)

    model = LinearRegression()
    y_predict_test = model.fit(X_poly_test, y_test).predict(X_poly_test)
    y_predict_train = model.fit(X_poly_train, y_train).predict(X_poly_train)

    mse_train_poly = mean_squared_error(y_train, y_predict_train)
    mse_test_poly = mean_squared_error(y_test, y_predict_test)
    return mse_train_poly, mse_test_poly

test_results = []
training_results =[]
for i in range(0,11):
    mse_train_poly, mse_test_poly = calcWithPol(i)
    test_results.append(mse_test_poly)
    training_results.append(mse_train_poly)

plt.plot(range(0,11), training_results, label='MSE training data', color='orange')
plt.plot(range(0,11), test_results, label='MSE test data', color='green')
plt.legend()
plt.show()
```